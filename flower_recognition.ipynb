{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "#import keras\n",
    "\n",
    "# Helper libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import cv2\n",
    "import pandas\n",
    "import os\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['daisy', 'roses', 'sunflowers', 'tulips']\n",
      "['random']\n"
     ]
    }
   ],
   "source": [
    "#Besoin de mettre le path absolu vers le dataset (normalement pr√©sent dans le projet)\n",
    "data = \"C:\\\\Users\\\\alexa\\\\Desktop\\\\M1\\\\Python\\\\Perso\\\\GitHub\\\\Projet_S1_M1\\\\dataset\\\\flower_photos\"\n",
    "\n",
    "#METTRE UNE SEULE PHOTO DANS LE DOSSIER TEST\n",
    "#SINON LE MODELE FAIT DE LA MERDE\n",
    "data_test = \"C:\\\\Users\\\\alexa\\\\Desktop\\\\M1\\\\Python\\\\Perso\\\\GitHub\\\\Projet_S1_M1\\\\test\"\n",
    "\n",
    "# List out the directories inside the main input folder\n",
    "\n",
    "folders = os.listdir(data)\n",
    "folders_test = os.listdir(data_test)\n",
    "print(folders)\n",
    "print(folders_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the images and resize them to a 128*128 size\n",
    "# Also generate the corresponding labels\n",
    "\n",
    "image_names = []\n",
    "train_labels = []\n",
    "train_images = []\n",
    "\n",
    "size = 64,64\n",
    "\n",
    "for folder in folders:\n",
    "    for file in os.listdir(os.path.join(data,folder)):\n",
    "        if file.endswith(\"jpg\"):\n",
    "            image_names.append(os.path.join(data,folder,file))\n",
    "            train_labels.append(folder)\n",
    "            img = cv2.imread(os.path.join(data,folder,file))\n",
    "            im = cv2.resize(img,size)\n",
    "            train_images.append(im)\n",
    "        else:\n",
    "            continue\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names_test = []\n",
    "train_labels_test = []\n",
    "train_images_test = []\n",
    "\n",
    "size = 64,64\n",
    "\n",
    "for folder in folders_test:\n",
    "    for file in os.listdir(os.path.join(data_test,folder)):\n",
    "        if file.endswith(\"jpg\"):\n",
    "            image_names_test.append(os.path.join(data_test,folder,file))\n",
    "            img_test = cv2.imread(os.path.join(data_test,folder,file))\n",
    "            im_test = cv2.resize(img_test,size)\n",
    "            train_images_test.append(im_test)\n",
    "        else:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 1022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow('image', im_test)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2643\n",
      "1\n",
      "(2643, 64, 64, 3)\n",
      "(1, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "#Check if we have the correct number of photos and if the resize is correct\n",
    "print(len(train_images))\n",
    "print(len(train_images_test))\n",
    "# Transform the image array to a numpy type and check if we have to correct data\n",
    "\n",
    "train = np.array(train_images)\n",
    "train_test = np.array(train_images_test)\n",
    "\n",
    "print(train.shape)\n",
    "print(train_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the RGB values between 0 and 1\n",
    "train = train.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels\n",
    "label_dummies = pandas.get_dummies(train_labels)\n",
    "\n",
    "labels =  label_dummies.values.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['daisy' 'dandelion' 'roses' 'sunflowers' 'tulips'] \n",
      " [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(pandas.unique(train_labels), \"\\n\", pandas.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the labels and images randomly for better results\n",
    "union_list = list(zip(train, labels))\n",
    "random.shuffle(union_list)\n",
    "train,labels = zip(*union_list)\n",
    "\n",
    "# Convert the shuffled list to numpy array type\n",
    "train = np.array(train)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train, labels, test_size=0.25, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop a sequential model using tensorflow keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(8,kernel_size=3,strides=2,activation='relu',padding='valid',input_shape=(64,64,3) ),\n",
    "    keras.layers.Conv2D(16,kernel_size=3,strides=2,activation='relu',padding='valid' ),          \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='tanh'),\n",
    "    keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_52 (Conv2D)           (None, 31, 31, 8)         224       \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 15, 15, 16)        1168      \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 3600)              0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 128)               460928    \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 462,965\n",
      "Trainable params: 462,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compute the model parameters\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.001), #tf.train.AdamOptimizer()', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1982 samples, validate on 661 samples\n",
      "Epoch 1/5\n",
      "1982/1982 [==============================] - 2s 1ms/sample - loss: 0.5581 - accuracy: 0.8047 - val_loss: 0.5618 - val_accuracy: 0.8094\n",
      "Epoch 2/5\n",
      "1982/1982 [==============================] - 1s 703us/sample - loss: 0.4298 - accuracy: 0.8547 - val_loss: 0.5298 - val_accuracy: 0.8139\n",
      "Epoch 3/5\n",
      "1982/1982 [==============================] - 1s 701us/sample - loss: 0.3073 - accuracy: 0.9026 - val_loss: 0.5426 - val_accuracy: 0.8154\n",
      "Epoch 4/5\n",
      "1982/1982 [==============================] - 1s 699us/sample - loss: 0.2167 - accuracy: 0.9374 - val_loss: 0.5543 - val_accuracy: 0.7912\n",
      "Epoch 5/5\n",
      "1982/1982 [==============================] - 1s 710us/sample - loss: 0.1597 - accuracy: 0.9637 - val_loss: 0.5595 - val_accuracy: 0.7837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x291930fea90>"
      ]
     },
     "execution_count": 933,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model  with 5 epochs \n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "#Get the data again\n",
    "img_gen_aug=keras.preprocessing.image.ImageDataGenerator(zoom_range=0.1,\n",
    "                                                     rotation_range=5,\n",
    "                                                     rescale=1/255,\n",
    "                                                     shear_range=5,\n",
    "                                                     width_shift_range=8,\n",
    "                                                     height_shift_range=8,                                    \n",
    "                                                     validation_split=0.1)\n",
    "\n",
    "img_dirit=img_gen_aug.flow_from_directory(data_test,target_size=size,\n",
    "                                      class_mode=\"sparse\",\n",
    "                                      batch_size=256,\n",
    "                                      subset=\"training\",\n",
    "                                      interpolation=\"bicubic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prediction from the model\n",
    "vals=[next(img_dirit) for i in range(8)]\n",
    "tx=np.concatenate([v[0] for v in vals])\n",
    "ty=np.concatenate([v[1] for v in vals])\n",
    "preds=model.predict(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted flower : \n",
      "Tulip\n",
      "[2.2819488e-01 7.5128027e-03 2.5551676e-06 7.6428974e-01]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "a = np.argmax(preds[i])\n",
    "print(\"Predicted flower : \")\n",
    "if a == 0:\n",
    "    print(\"Daisy\")\n",
    "elif a == -1:\n",
    "    print(\"Dandelion\")\n",
    "elif a == 1:\n",
    "    print(\"Rose\")\n",
    "elif a == 2:\n",
    "    print(\"Sunflower\")\n",
    "elif a == 3:\n",
    "    print(\"Tulip\")\n",
    "    \n",
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.json\", \"w\") as file:\n",
    "    file.write(model.to_json())\n",
    "model.save_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model from the 2 files we saved\n",
    "with open('model.json') as json_file:\n",
    "    json_config = json_file.read()\n",
    "model = keras.models.model_from_json(json_config)\n",
    "model.load_weights('weights.h5')\n",
    "\n",
    "# Note that the optimizer was not preserved so we can't train the model anymore (or we have to do model.compile again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
